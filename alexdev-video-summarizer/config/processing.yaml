# alexdev-video-summarizer Configuration
# Scene-based institutional knowledge extraction settings
# DEMUCS OPTIMIZATION: With clean audio separation, each service focuses only on its expertise

# Application Settings
app:
  name: "alexdev-video-summarizer"
  version: "1.0.0"
  description: "Scene-based institutional knowledge extraction"

# Directory Configuration
paths:
  input_dir: "input"
  output_dir: "output" 
  build_dir: "build"
  models_dir: "models"
  logs_dir: "logs"
  temp_dir: "temp"

# FFmpeg Settings
ffmpeg:
  # Audio extraction settings (optimized for Whisper)
  audio:
    sample_rate: 22050
    bit_depth: 16
    channels: 2
    format: "wav"
    codec: "pcm_s16le"
  
  # Video extraction settings  
  video:
    codec: "libx264"
    quality: "high"
    preserve_resolution: true
    preserve_framerate: true
    
  # Hardware acceleration (auto-detect)
  hardware_accel: "auto"
  thread_count: "auto"

# Scene Detection Settings
scene_detection:
  # PySceneDetect configuration - TUNED FOR 30s/8-scene DETECTION
  threshold: 15.0              # LOWERED: More sensitive for short video transitions  
  min_scene_length: 0.3        # LOWERED: Allow very short scenes (0.3s minimum)
  downscale_factor: 1          # 1 = full resolution analysis for accuracy
  frame_skip: 0                # Analyze every frame for maximum accuracy
  
  # Fallback settings
  fallback_to_time_based: true
  time_based_scene_length: 120  # 2-minute fallback scenes

# Semantic Scene Detection Settings - VLM-enhanced boundary optimization
semantic_scene_detection:
  # Minimum scene duration constraints
  min_scene_duration: 2.0             # Minimum seconds for normal scenes
  rapid_cut_threshold: 1.0            # Scenes shorter than this marked as rapid cuts
  
  # Algorithm control
  max_stabilization_iterations: 5     # Maximum iterations for boundary stabilization
  enable_cross_scene_analysis: true   # Enable Phase 1: cross-scene merging
  enable_intra_scene_analysis: true   # Enable Phase 2: intra-scene splitting
  
  # VLM semantic thresholds
  merge_threshold: 0.75               # Similarity required to merge adjacent scenes (higher = more conservative)
  split_threshold: 0.25               # Difference required to split within scenes (lower = more conservative)
  
  # Performance optimization
  sample_interval_ratio: 5            # Sample frames within scenes (scene_duration / ratio)
  max_samples_per_scene: 6            # Maximum sample points per scene for intra-scene analysis
  
  # Quality control
  boundary_confidence_threshold: 0.6  # Minimum confidence for boundary changes
  preserve_original_on_failure: true  # Use PySceneDetect boundaries if semantic analysis fails

# GPU Pipeline Configuration  
gpu_pipeline:
  # Sequential processing to prevent CUDA conflicts
  sequential_processing: true
  memory_cleanup: true
  max_gpu_memory_usage: 0.9    # 90% GPU memory limit
  
    
  # InternVL3 VLM Settings - Replaces YOLO+EasyOCR+OpenCV
  # RAPID MODEL TESTING: Change model_name and model_path to test different models
  # Model names will automatically appear in filenames, logs, and metadata
  internvl3:
    model_name: "InternVL3_5-2B"
    model_path: "OpenGVLab/InternVL3_5-2B"
    device: "cuda"
    confidence_threshold: 0.7
    max_tokens: 512
    frame_sample_rate: 3
    
    # Generation Configuration - Controls inference behavior
    generation_config:
      num_beams: 1
      max_new_tokens: 512
      do_sample: false
      temperature: 0.0
      repetition_penalty: 1.0
      max_length: 8192
      top_p: 0.9
    
    # VLM analysis modes
    comprehensive_analysis: true       # Full scene understanding
    text_extraction: true             # Extract all readable text from scenes
    change_detection: true            # Detect changes between scenes
    
    # Additional VLM settings
    max_input_tiles: 6                # Maximum image tiles for dynamic preprocessing
    use_thumbnail: false              # Use thumbnail in multi-tile processing
    
    # Production-ready VLM processing only
  
    
  # Whisper Settings (Hybrid: Original Whisper GPU + WhisperX features)
  whisper:
    model_size: "large-v2"      # large-v2 for best accuracy
    device: "cuda"              # GPU with original Whisper
    language: "en"              # Force English to prevent translation
    
    # Hybrid mode: Use original Whisper with WhisperX features
    use_original_whisper: true  # Fallback to original Whisper for GPU compatibility
    enable_silero_vad: true     # Add Silero VAD for better segmentation
    
    # VAD Settings (Voice Activity Detection) - Optimized for fast-paced ads
    vad_threshold: 0.3          # Voice activity detection sensitivity
    chunk_threshold: 1.0        # Gap required to split chunks (1s for fast-paced ads)
    
    # Word timestamps (original Whisper supports this)
    enable_word_timestamps: true # Enable word-level timestamps
    enable_word_alignment: true  # Enable WhisperX word alignment when available
    
    # Advanced Chunking for Advertisement Content
    chunking_strategy: "multi_strategy"  # Options: 'gap_based', 'duration_based', 'multi_strategy'
    max_chunk_duration: 20.0    # Maximum chunk duration (seconds) - prevents very long chunks
    min_chunk_duration: 5.0     # Minimum chunk duration (seconds) - merges very short chunks
    energy_based_splitting: true # Enable energy-based splitting at natural breaks
    
    # Sequential model loading for GPU memory management
    sequential_model_loading: true  # Load/unload diarization model separately to manage GPU memory
    
    # Speaker Diarization
    hft_encoded: "bDACrQSIdR1zeISygygiVeZgUfMbTzGh_fh"
    enable_diarization: true    # Enable speaker diarization when token available
    max_speakers: 10            # Maximum number of speakers to detect
    min_speakers: 1             # Minimum number of speakers to detect

# ML Models Configuration - Phase 7 (ML Model Integration)
ml_models:
  # ML Emotion Detection (replaces heuristic emotion detection) - DISABLED: Model has config issues
  emotion:
    model_name: "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"  # BROKEN: Missing vocab files
    device: "cuda"              # Use GPU for ML emotion detection (sequential with Whisper)
    cache_models: true          # Cache models for better performance
    enabled: false              # DISABLED: wav2vec2 model has loading issues
    
    # Analysis parameters
    window_size: 5.0            # 5 second windows for stable emotion detection (larger than heuristic)
    overlap: 2.5                # 2.5 second overlap for smooth transitions
    confidence_threshold: 0.6   # ML model confidence threshold for emotion changes
    
    # Model loading settings
    lazy_loading: true          # Load models only when needed
    sequential_gpu_usage: true  # Coordinate with other GPU models (Whisper, etc.)
  
  # ML Audio Event Classification (replaces heuristic sound effects detection) - DISABLED: Not better than LibROSA
  ast_audio_events:
    model_name: "MIT/ast-finetuned-audioset-10-10-0.4593"  # AST-AudioSet model (527 classes, 95% accuracy)
    device: "cuda"              # Use GPU for ML audio classification (sequential processing)
    cache_models: true          # Cache models for better performance
    enabled: false              # DISABLED: Not better than existing LibROSA analysis
    
    # Analysis parameters
    window_size: 10.0           # 10 second windows for AudioSet analysis (longer for sound context)
    overlap: 5.0                # 5 second overlap for sound event continuity
    confidence_threshold: 0.3   # Lower threshold for sound effects detection (more sensitive)
    
    # Model loading settings
    lazy_loading: true          # Load models only when needed
    sequential_gpu_usage: true  # Coordinate with other GPU models (Whisper, emotion, etc.)
  
  # ML Music Understanding (replaces heuristic music features + genre classification) - DISABLED: Not better than PyAudio
  mert_music:
    model_name: "m-a-p/MERT-v1-330M"  # MERT music understanding model (50+ genres, comprehensive music analysis)
    device: "cuda"              # Use GPU for ML music understanding (sequential processing)
    cache_models: true          # Cache models for better performance
    enabled: false              # DISABLED: Not better than existing PyAudio + LibROSA analysis
    
    # Analysis parameters
    window_size: 15.0           # 15 second windows for music analysis (longer for musical context)
    overlap: 7.5                # 7.5 second overlap for music continuity
    confidence_threshold: 0.4   # Music classification confidence threshold
    
    # Model loading settings
    lazy_loading: true          # Load models only when needed
    sequential_gpu_usage: true  # Coordinate with other GPU models (Whisper, emotion, AST, etc.)

# CPU Pipeline Configuration
cpu_pipeline:
  # Parallel processing settings
  max_workers: 3               # Thread pool size
  timeout_seconds: 300         # 5 minute timeout per tool
  
      
  # LibROSA Settings  
  librosa:
    sample_rate: 22050
    hop_length: 512
    n_mfcc: 13
    
    # Smart Music Segmentation (detects style changes automatically)
    enable_music_segmentation: true   # Enable music-based analysis segmentation
    adaptive_segmentation: true       # Use smart boundary detection vs fixed-time
    
    # Smart segmentation parameters
    min_segment_length: 3.0          # Minimum segment duration (seconds)
    max_segment_length: 30.0         # Maximum segment duration (seconds)
    
    # Feature change detection thresholds
    spectral_change_threshold: 0.3   # Sensitivity for musical style changes
    tempo_change_threshold: 10.0     # BPM change threshold for rhythm detection
    harmonic_change_threshold: 0.4   # Chord/key change sensitivity
    energy_change_threshold: 0.5     # Volume/dynamics change sensitivity
    
    # Fixed segmentation fallback (when smart detection disabled)
    segment_length: 10.0             # Fixed segment duration (seconds)
    segment_overlap: 2.0             # Overlap between segments (seconds)
    
  # pyAudioAnalysis Settings - UPDATED: ML emotion detection enabled
  pyaudioanalysis:
    window_size: 0.050          # 50ms windows
    step_size: 0.025            # 25ms step size
    features: "all"             # Extract all 68 features (numerical only)
    output_mode: "numerical"    # CHANGED: Only numerical - fake interpretive removed
    # REMOVED interpretive mode - was fake heuristic analysis with arbitrary thresholds
    # Timeline services now handle real event-based analysis
    
    # ML emotion detection configuration (Phase 7 enhancement)
    use_ml_emotion: false       # DISABLED: Enable ML emotion detection (85-90% accuracy vs 30% heuristic)
    emotion_change_threshold: 0.6  # ML confidence threshold for emotion change events
    energy_change_threshold: 0.15  # Keep for audio event detection
    genre_confidence_threshold: 0.6 # Keep for environment classification
    
    # Emotion detection window settings
    analysis_window: 3.0        # 3 second windows for emotion analysis
    overlap_window: 1.5         # 1.5 second overlap
    
    # Thresholds for heuristic features (still used for audio events and environment detection)
    high_energy_threshold: 0.3
    medium_energy_threshold: 0.15
    high_spectral_centroid: 3000
    medium_spectral_centroid: 2000
    low_spectral_centroid: 1500
    high_zero_crossing: 0.15
    medium_zero_crossing: 0.1

# Enhanced Timeline Merger Settings
timeline_merger:
  # LibROSA Speech Artifact Filtering
  enable_speech_artifact_filtering: true   # Filter LibROSA events that correlate with speech
  
  # PyAudio-based filtering (distance from PyAudio emotion change events)
  pyaudio_distance_threshold: 0.7         # LibROSA events >0.7s from PyAudio events are likely valid musical events
  
  # Boundary exception rules - preserve events near structural boundaries
  boundary_exception_distance: 0.5        # Events within 0.5s of boundaries are preserved regardless of other filters
  preserve_boundary_events: true          # Enable boundary exception logic
  
  # Alternative filtering methods (if PyAudio filtering disabled)
  word_distance_threshold: 0.15           # LibROSA events >0.15s from words (less reliable)
  enable_event_clustering_filter: false   # Filter dense clusters of events (experimental)

# Error Handling & Circuit Breaker
error_handling:
  # Circuit breaker configuration
  circuit_breaker_threshold: 3  # Abort after 3 consecutive failures
  
  # Retry settings
  max_retries: 1
  retry_delay: 5               # seconds
  
  # Error recovery
  continue_on_tool_failure: false  # Fail-fast approach
  cleanup_failed_artifacts: true
  
  # GPU error handling
  gpu_memory_recovery: true
  cuda_cache_cleanup: true

# Performance Settings
performance:
  # Processing targets
  target_processing_time: 600  # 10 minutes per video
  max_processing_time: 1800    # 30 minute timeout
  
  # Memory management
  force_gc_after_video: true
  gpu_memory_cleanup: true
  
  # Progress reporting
  progress_update_interval: 5   # seconds
  detailed_progress: true

# Output Settings
output:
  # Knowledge base format
  format: "markdown"
  scene_by_scene: true
  include_metadata: true
  include_cross_references: true
  
  # File naming
  filename_pattern: "{video_name}.md"
  master_index: "INDEX.md"
  
  # Content options
  include_timestamps: true
  include_confidence_scores: false
  max_scene_description_length: 500

# Logging Configuration
logging:
  level: "INFO"                # DEBUG, INFO, WARNING, ERROR
  file_logging: true
  console_logging: true
  log_format: "detailed"       # simple, detailed
  
  # Log file settings
  max_log_size: "10MB"
  backup_count: 3
  
  # Component logging levels
  services:
    ffmpeg: "INFO"
    scene_detection: "INFO"
    gpu_pipeline: "INFO"
    cpu_pipeline: "INFO"
    knowledge_generator: "INFO"

# DEMUCS OPTIMIZATION: Service-Specific Analysis Focus
audio_optimization:
  # LibROSA on no_vocals.wav - Full music analysis (everything on)
  librosa_music:
    analyze_tempo_changes: true        # Essential for music
    analyze_harmonic_shifts: true      # Essential for music  
    analyze_energy_transitions: true   # Keep - valuable for music dynamics
    analyze_onset_events: true         # Keep - valuable for music analysis
    analyze_structural_segments: true  # Essential for music structure
    
  # PyAudio on vocals.wav - Focus on vocal emotion only (Whisper+pyannote handles speaker detection)
  pyaudio_voice:
    analyze_emotion_changes: true     # Valuable for voice analysis
    analyze_sound_effects: false     # Skip - no sound effects in clean vocals
    analyze_music_features: false    # Skip - vocals track has no music
    analyze_genre_classification: false # Skip - not applicable to vocals
    
  # PyAudio on no_vocals.wav - Focus on sound effects, music features, and genre
  pyaudio_music:
    analyze_emotion_changes: false   # Skip - not applicable to music
    analyze_sound_effects: true      # Keep - valuable for institutional knowledge
    analyze_music_features: true     # Essential for music classification
    analyze_genre_classification: true # Valuable for institutional categorization

# Development Settings (for testing)
development:
  skip_model_loading: false    # PRODUCTION: Load all AI models for real processing
  skip_ffmpeg_check: false     # FFmpeg is now installed and ready
  fast_mode: false             # PRODUCTION: Full quality processing
  debug_artifacts: true        # Keep intermediate files for debugging